{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.tokens import DocBin\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../raw_data/data_prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BB', 'AAA', 'AA', 'A', 'BBB', 'B', 'C'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = df['rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "syms = ['A', 'B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_a = ['AAA', 'AA+', 'A', 'A-', 'A+', 'AA', 'AA-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_b = ['BB', 'BB+', 'BBB', 'BBB+', 'B-', 'B+', 'BB-', 'B', 'BBB-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((960, 8), (240, 8))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "dev = dev.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_cat = spacy.load('../cat/cat_model/model-best/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sym = spacy.load('../sym/sym_model/model-best/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sub_a = spacy.load('../subsym/subsym_a/subsym_model/model-best/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sub_b = spacy.load('../subsym/subsym_b/subsym_model/model-best/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prection(text, model):\n",
    "    doc = model(text)\n",
    "    scores = doc.cats\n",
    "    return max(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prection_rating(row, model1, model2):\n",
    "    if row['sym'] == 'A':     \n",
    "        doc = model1(row['tokenized_str'])\n",
    "        scores = doc.cats\n",
    "        return max(scores, key=scores.get)\n",
    "    else:\n",
    "        doc = model2(row['tokenized_str'])\n",
    "        scores = doc.cats\n",
    "        return max(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['y'] = dev['clear_text'].apply(lambda x: get_prection(x, nlp_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['sym'] = dev['clear_text'].apply(lambda x: get_prection(x, nlp_sym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['rating_y'] = dev.apply(lambda x: get_prection_rating(x, nlp_sub_a, nlp_sub_b), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B+', 'A+', 'AA-', 'BB-', 'A-', 'BB', 'AAA', 'AA+', 'BBB+', 'BBB',\n",
       "       'B-', 'AA', 'BBB-', 'A', 'BB+', 'B'], dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev['rating_y'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>pr_txt</th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>clear_text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tokenized_str</th>\n",
       "      <th>y</th>\n",
       "      <th>sym</th>\n",
       "      <th>rating_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, Id, pr_txt, category, rating, clear_text, tokenized, tokenized_str, y, sym, rating_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.query('rating_y == \"C\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          BB       0.75      0.92      0.83        83\n",
      "         AAA       0.79      0.71      0.75        38\n",
      "          AA       0.91      0.91      0.91        32\n",
      "           A       1.00      0.86      0.92         7\n",
      "         BBB       0.77      0.87      0.82        23\n",
      "           B       0.92      0.67      0.77        54\n",
      "           C       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.82       240\n",
      "   macro avg       0.88      0.80      0.83       240\n",
      "weighted avg       0.83      0.82      0.81       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dev['category'], dev['y'], target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          BB       1.00      1.00      1.00        18\n",
      "         AAA       0.93      1.00      0.97        28\n",
      "         AA+       0.90      1.00      0.95        37\n",
      "           A       1.00      1.00      1.00        13\n",
      "         BB+       1.00      1.00      1.00        13\n",
      "          A-       1.00      1.00      1.00        12\n",
      "          A+       1.00      1.00      1.00        32\n",
      "         BBB       1.00      1.00      1.00         2\n",
      "        BBB+       1.00      1.00      1.00         4\n",
      "          B-       1.00      1.00      1.00         1\n",
      "          AA       1.00      0.89      0.94         9\n",
      "          B+       0.90      1.00      0.95         9\n",
      "         BB-       1.00      1.00      1.00         5\n",
      "         AA-       0.95      1.00      0.97        19\n",
      "           C       1.00      0.83      0.90        23\n",
      "           B       1.00      1.00      1.00        12\n",
      "        BBB-       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.97       240\n",
      "   macro avg       0.92      0.92      0.92       240\n",
      "weighted avg       0.96      0.97      0.96       240\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wilte\\OneDrive\\Рабочий стол\\projects\\rating-agency-analyser\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\wilte\\OneDrive\\Рабочий стол\\projects\\rating-agency-analyser\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\wilte\\OneDrive\\Рабочий стол\\projects\\rating-agency-analyser\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dev['rating'], dev['rating_y'], target_names=ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9092693461243028"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f1_score(dev['category'], dev['y'], average='weighted') * 0.35) + (f1_score(dev['rating'], dev['rating_y'], average='weighted') * 0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6241524661891374"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
